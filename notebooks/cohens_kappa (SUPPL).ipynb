{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Projecten\\Sepsis\\src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src\n",
    "%pwd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Patient', 'Shock', 'Death', 'Shock_Int_1', 'Mortality_Int_1',\n",
      "       'Shock_Int_2', 'Mortality_Int_2', 'Shock_Int_3', 'Mortality_Int_3',\n",
      "       'Shock_Int_4', 'Mortality_Int_4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "\n",
    "cols = list(chain.from_iterable([[\"Shock_Int_{}\".format(x), \"Mortality_Int_{}\".format(x)] for x in range(1,5)]))\n",
    "cols = ['Patient', 'Shock', 'Death'] + cols\n",
    "df_internist = pd.read_excel('../data/internisten/20190412_Ruwe data internisten.xlsx', skiprows=1, names=cols, nrows=100)\n",
    "print(df_internist.columns)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score\n",
    "from scipy import interp\n",
    "\n",
    "internist_shock_preds = []\n",
    "for cols in ['Shock_Int_1', 'Shock_Int_2', 'Shock_Int_3', 'Shock_Int_4']:\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(df_internist['Shock'], df_internist[cols])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    internist_shock_preds.append([fpr[1], tpr[1], roc_auc])\n",
    "\n",
    "internist_death_preds = []\n",
    "for cols in ['Mortality_Int_1', 'Mortality_Int_2', 'Mortality_Int_3', 'Mortality_Int_4']: \n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(df_internist['Death'], df_internist[cols])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    internist_death_preds.append([fpr[1], tpr[1], roc_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mcohen_kappa_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
       "\n",
       "This function computes Cohen's kappa [1]_, a score that expresses the level\n",
       "of agreement between two annotators on a classification problem. It is\n",
       "defined as\n",
       "\n",
       ".. math::\n",
       "    \\kappa = (p_o - p_e) / (1 - p_e)\n",
       "\n",
       "where :math:`p_o` is the empirical probability of agreement on the label\n",
       "assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
       "the expected agreement when both annotators assign labels randomly.\n",
       ":math:`p_e` is estimated using a per-annotator empirical prior over the\n",
       "class labels [2]_.\n",
       "\n",
       "Read more in the :ref:`User Guide <cohen_kappa>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y1 : array, shape = [n_samples]\n",
       "    Labels assigned by the first annotator.\n",
       "\n",
       "y2 : array, shape = [n_samples]\n",
       "    Labels assigned by the second annotator. The kappa statistic is\n",
       "    symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
       "\n",
       "labels : array, shape = [n_classes], optional\n",
       "    List of labels to index the matrix. This may be used to select a\n",
       "    subset of labels. If None, all labels that appear at least once in\n",
       "    ``y1`` or ``y2`` are used.\n",
       "\n",
       "weights : str, optional\n",
       "    Weighting type to calculate the score. None means no weighted;\n",
       "    \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "kappa : float\n",
       "    The kappa statistic, which is a number between -1 and 1. The maximum\n",
       "    value means complete agreement; zero or lower means chance agreement.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
       "       Educational and Psychological Measurement 20(1):37-46.\n",
       "       doi:10.1177/001316446002000104.\n",
       ".. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
       "       computational linguistics\". Computational Linguistics 34(4):555-596.\n",
       "       <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_\n",
       ".. [3] `Wikipedia entry for the Cohen's kappa.\n",
       "        <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\envs\\ml_main\\lib\\site-packages\\sklearn\\metrics\\_classification.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "?cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5233968804159446"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Mortality_Int_1', 'Mortality_Int_2', 'Mortality_Int_3', 'Mortality_Int_4']\n",
    "\n",
    "cohen_kappa_score(df_internist[cols[0]], df_internist[cols[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mortality_Int_1 versus Mortality_Int_1 - Kappa: 1.00\n",
      "Mortality_Int_1 versus Mortality_Int_2 - Kappa: 0.52\n",
      "Mortality_Int_1 versus Mortality_Int_3 - Kappa: 0.54\n",
      "Mortality_Int_1 versus Mortality_Int_4 - Kappa: 0.46\n",
      "Mortality_Int_2 versus Mortality_Int_1 - Kappa: 0.52\n",
      "Mortality_Int_2 versus Mortality_Int_2 - Kappa: 1.00\n",
      "Mortality_Int_2 versus Mortality_Int_3 - Kappa: 0.61\n",
      "Mortality_Int_2 versus Mortality_Int_4 - Kappa: 0.67\n",
      "Mortality_Int_3 versus Mortality_Int_1 - Kappa: 0.54\n",
      "Mortality_Int_3 versus Mortality_Int_2 - Kappa: 0.61\n",
      "Mortality_Int_3 versus Mortality_Int_3 - Kappa: 1.00\n",
      "Mortality_Int_3 versus Mortality_Int_4 - Kappa: 0.63\n",
      "Mortality_Int_4 versus Mortality_Int_1 - Kappa: 0.46\n",
      "Mortality_Int_4 versus Mortality_Int_2 - Kappa: 0.67\n",
      "Mortality_Int_4 versus Mortality_Int_3 - Kappa: 0.63\n",
      "Mortality_Int_4 versus Mortality_Int_4 - Kappa: 1.00\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Loop through all pairs\n",
    "for pair in product(cols, repeat=2):\n",
    "    kappa = cohen_kappa_score(df_internist[pair[0]], df_internist[pair[1]])\n",
    "    print('{} versus {} - Kappa: {:.2f}'.format(pair[0],\n",
    "                                                pair[1],\n",
    "                                                kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mortality_Int_1', 'Mortality_Int_2', 'Mortality_Int_3', 'Mortality_Int_4']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
